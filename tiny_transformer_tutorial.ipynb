{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.x"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VdJGh2dSf181"},"source":["# Tiny Transformer Language Model (Character-Level)\n","\n","This notebook implements a **minimal Transformer language model** from scratch for **educational purposes**:\n","\n","- Character-level dataset\n","- Token + positional embeddings\n","- Masked self-attention (single head)\n","- Transformer block (attention + feedforward + residual + layer norm)\n","- Tiny training loop\n","- Text generation\n","\n","Feel free to modify the sample text, model size, or training steps for experiments."],"id":"VdJGh2dSf181"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjE_9VFGf185","executionInfo":{"status":"ok","timestamp":1763394439204,"user_tz":420,"elapsed":7190,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"6fff5d8c-e32a-4434-a63b-f9a02ea82fad"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["# Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import random\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using device:', device)"],"id":"hjE_9VFGf185"},{"cell_type":"markdown","metadata":{"id":"hqMHsSuHf188"},"source":["## 1. Build a tiny character-level dataset\n","\n","We use a short text, build a character vocabulary, and create (context, next-character) pairs.\n"],"id":"hqMHsSuHf188"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ8HkH9zf189","executionInfo":{"status":"ok","timestamp":1763394443349,"user_tz":420,"elapsed":117,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"af8bdb58-83c4-4a0c-baea-8eefb88e381d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: [' ', '.', 'H', 'T', 'a', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'w']\n","vocab_size = 19\n","Encoded text (first 50 tokens): [2, 6, 10, 10, 13, 0, 18, 13, 15, 10, 5, 1, 0, 3, 8, 9, 16, 0, 9, 16, 0, 4, 0, 16, 9, 11, 14, 10, 6, 0, 17, 15, 4, 12, 16, 7, 13, 15, 11, 6, 15, 0, 5, 6, 11, 13, 1]\n","Example batch x: tensor([[16,  0,  4,  0, 16,  9, 11, 14, 10,  6,  0, 17, 15,  4, 12, 16],\n","        [16,  0,  9, 16,  0,  4,  0, 16,  9, 11, 14, 10,  6,  0, 17, 15]])\n","Example batch y: tensor([[ 0,  4,  0, 16,  9, 11, 14, 10,  6,  0, 17, 15,  4, 12, 16,  7],\n","        [ 0,  9, 16,  0,  4,  0, 16,  9, 11, 14, 10,  6,  0, 17, 15,  4]])\n"]}],"source":["# Sample text data (you can replace this with anything you like)\n","text = \"Hello world. This is a simple transformer demo.\"\n","\n","# 1) Build vocabulary\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print('Vocabulary:', chars)\n","print('vocab_size =', vocab_size)\n","\n","char_to_idx = {ch: i for i, ch in enumerate(chars)}\n","idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n","\n","# 2) Encode full text as integer tensor\n","data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n","print('Encoded text (first 50 tokens):', data[:50].tolist())\n","\n","# 3) Create training examples (context -> next char)\n","block_size = 16  # maximum context length\n","\n","def get_batch(batch_size=32):\n","    \"\"\"Return a batch of (x, y) where:\n","    - x: [batch_size, block_size]\n","    - y: [batch_size, block_size]\n","    y is x shifted by one position (next-character prediction).\n","    \"\"\"\n","    idx = torch.randint(len(data) - block_size - 1, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in idx])\n","    y = torch.stack([data[i+1:i+1+block_size] for i in idx])\n","    return x.to(device), y.to(device)\n","\n","# Quick sanity check\n","xb, yb = get_batch(batch_size=2)\n","print('Example batch x:', xb)\n","print('Example batch y:', yb)"],"id":"pZ8HkH9zf189"},{"cell_type":"markdown","metadata":{"id":"-KWVryKRf18-"},"source":["## 2. Token + positional embedding layer\n","\n","We embed token IDs and add positional embeddings (so the model knows *where* each token is in the sequence)."],"id":"-KWVryKRf18-"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHdD38vnf18-","executionInfo":{"status":"ok","timestamp":1763394465501,"user_tz":420,"elapsed":46,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"b691dd5f-af67-447b-8cf5-af10f606be54"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding output shape: torch.Size([2, 16, 8])\n"]}],"source":["class TokenAndPositionalEmbedding(nn.Module):\n","    def __init__(self, vocab_size, d_model, max_len):\n","        super().__init__()\n","        self.token_emb = nn.Embedding(vocab_size, d_model)\n","        self.pos_emb = nn.Embedding(max_len, d_model)\n","\n","    def forward(self, x):\n","        \"\"\"x: [batch_size, seq_len] (token indices)\n","        returns: [batch_size, seq_len, d_model]\n","        \"\"\"\n","        B, T = x.shape\n","        token_vecs = self.token_emb(x)  # [B, T, d_model]\n","        positions = torch.arange(T, device=x.device).unsqueeze(0)  # [1, T]\n","        pos_vecs = self.pos_emb(positions)  # [1, T, d_model]\n","        return token_vecs + pos_vecs\n","\n","# Tiny test\n","embed_test = TokenAndPositionalEmbedding(vocab_size, d_model=8, max_len=block_size).to(device)\n","out = embed_test(xb)\n","print('Embedding output shape:', out.shape)"],"id":"mHdD38vnf18-"},{"cell_type":"markdown","metadata":{"id":"rKdlICmuf18_"},"source":["## 3. Single-head masked self-attention\n","\n","This is the core idea of a Transformer: each position attends to previous positions (causal mask)."],"id":"rKdlICmuf18_"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctZnBqqff19A","executionInfo":{"status":"ok","timestamp":1763394469980,"user_tz":420,"elapsed":74,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"80c578aa-6d0c-4aad-e317-cdde92c17651"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention output shape: torch.Size([2, 16, 8])\n"]}],"source":["class SelfAttentionHead(nn.Module):\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.key   = nn.Linear(d_model, d_model, bias=False)\n","        self.query = nn.Linear(d_model, d_model, bias=False)\n","        self.value = nn.Linear(d_model, d_model, bias=False)\n","\n","    def forward(self, x):\n","        \"\"\"x: [B, T, d_model]\"\"\"\n","        B, T, C = x.shape\n","        K = self.key(x)   # [B, T, C]\n","        Q = self.query(x) # [B, T, C]\n","        V = self.value(x) # [B, T, C]\n","\n","        # Scaled dot-product attention\n","        scores = Q @ K.transpose(-2, -1)  # [B, T, T]\n","        scores = scores / math.sqrt(C)\n","\n","        # Causal mask: prevent looking into the future\n","        mask = torch.tril(torch.ones(T, T, device=x.device))\n","        scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","        attn = F.softmax(scores, dim=-1)  # [B, T, T]\n","\n","        # Weighted sum of values\n","        out = attn @ V  # [B, T, C]\n","        return out\n","\n","# Quick test\n","attn_test = SelfAttentionHead(d_model=8).to(device)\n","attn_out = attn_test(out)\n","print('Attention output shape:', attn_out.shape)"],"id":"ctZnBqqff19A"},{"cell_type":"markdown","metadata":{"id":"4sWeK2jjf19B"},"source":["## 4. Transformer block (Attention + Feedforward)\n","\n","Each block:\n","1. LayerNorm → Self-Attention → Residual\n","2. LayerNorm → Feedforward (MLP) → Residual"],"id":"4sWeK2jjf19B"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2ciOSqyf19D","executionInfo":{"status":"ok","timestamp":1763394472948,"user_tz":420,"elapsed":23,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"aa700b1d-03f6-4b7e-b542-1dc67da53f4b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Block output shape: torch.Size([2, 16, 8])\n"]}],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(d_model)\n","        self.ln2 = nn.LayerNorm(d_model)\n","        self.attn = SelfAttentionHead(d_model)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model),\n","        )\n","\n","    def forward(self, x):\n","        # Self-attention with residual\n","        x_norm = self.ln1(x)\n","        attn_out = self.attn(x_norm)\n","        x = x + attn_out\n","\n","        # Feedforward with residual\n","        x_norm = self.ln2(x)\n","        ff_out = self.ff(x_norm)\n","        x = x + ff_out\n","        return x\n","\n","# Quick test\n","block_test = TransformerBlock(d_model=8, d_ff=16).to(device)\n","block_out = block_test(attn_out)\n","print('Block output shape:', block_out.shape)"],"id":"P2ciOSqyf19D"},{"cell_type":"markdown","metadata":{"id":"20romBpAf19E"},"source":["## 5. Full tiny Transformer language model\n","\n","We stack:\n","- Token+position embedding\n","- Several Transformer blocks\n","- Final linear layer to predict next-token logits"],"id":"20romBpAf19E"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RN5LSfXwf19E","executionInfo":{"status":"ok","timestamp":1763394476862,"user_tz":420,"elapsed":22,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"85e65c0f-1439-4e19-ad64-e2bcf32d12bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["TinyTransformerLM(\n","  (embed): TokenAndPositionalEmbedding(\n","    (token_emb): Embedding(19, 64)\n","    (pos_emb): Embedding(16, 64)\n","  )\n","  (blocks): ModuleList(\n","    (0-1): 2 x TransformerBlock(\n","      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (attn): SelfAttentionHead(\n","        (key): Linear(in_features=64, out_features=64, bias=False)\n","        (query): Linear(in_features=64, out_features=64, bias=False)\n","        (value): Linear(in_features=64, out_features=64, bias=False)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=64, out_features=128, bias=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=128, out_features=64, bias=True)\n","      )\n","    )\n","  )\n","  (ln_final): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=64, out_features=19, bias=True)\n",")\n","Logits shape: torch.Size([2, 16, 19])\n"]}],"source":["class TinyTransformerLM(nn.Module):\n","    def __init__(self, vocab_size, d_model=64, n_layers=2, d_ff=128, max_len=block_size):\n","        super().__init__()\n","        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n","        self.blocks = nn.ModuleList([\n","            TransformerBlock(d_model, d_ff) for _ in range(n_layers)\n","        ])\n","        self.ln_final = nn.LayerNorm(d_model)\n","        self.head = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, idx):\n","        \"\"\"idx: [B, T] token indices\n","        returns: [B, T, vocab_size] logits\n","        \"\"\"\n","        x = self.embed(idx)\n","        for block in self.blocks:\n","            x = block(x)\n","        x = self.ln_final(x)\n","        logits = self.head(x)\n","        return logits\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens=50):\n","        \"\"\"Autoregressive generation starting from idx [1, T].\"\"\"\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]  # use last context window\n","            logits = self(idx_cond)\n","            logits_last = logits[:, -1, :]  # last time step\n","            probs = F.softmax(logits_last, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat([idx, next_token], dim=1)\n","        return idx\n","\n","# Instantiate model\n","model = TinyTransformerLM(vocab_size).to(device)\n","print(model)\n","\n","# Test forward pass\n","logits_test = model(xb)\n","print('Logits shape:', logits_test.shape)"],"id":"RN5LSfXwf19E"},{"cell_type":"markdown","metadata":{"id":"qTatlYcXf19E"},"source":["## 6. Training loop\n","\n","We train with cross-entropy loss on next-character prediction. This is **tiny** and purely for demonstration, so don't expect amazing text quality."],"id":"qTatlYcXf19E"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vx6EW3TDf19F","executionInfo":{"status":"ok","timestamp":1763394506517,"user_tz":420,"elapsed":19348,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"2de0528e-c098-4064-edb4-b04f47c357f7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0, loss = 3.0165\n","step 100, loss = 0.1509\n","step 200, loss = 0.0669\n","step 300, loss = 0.0715\n","step 400, loss = 0.0643\n","step 500, loss = 0.0581\n","step 600, loss = 0.0551\n","step 700, loss = 0.0527\n","step 800, loss = 0.0509\n","step 900, loss = 0.0500\n"]}],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","num_steps = 1000  # keep small for demo; increase if you like\n","\n","for step in range(num_steps):\n","    model.train()\n","    xb, yb = get_batch(batch_size=32)\n","\n","    logits = model(xb)\n","    B, T, V = logits.shape\n","    loss = F.cross_entropy(logits.view(B*T, V), yb.view(B*T))\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if step % 100 == 0:\n","        print(f\"step {step}, loss = {loss.item():.4f}\")"],"id":"Vx6EW3TDf19F"},{"cell_type":"markdown","metadata":{"id":"DOgx9DLCf19F"},"source":["## 7. Text generation\n","\n","We seed the model with some initial characters and let it generate more, one character at a time."],"id":"DOgx9DLCf19F"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8yi5mtdVf19F","executionInfo":{"status":"ok","timestamp":1763394514294,"user_tz":420,"elapsed":84,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"ea176800-1086-41d1-882f-9e920bd4706a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Seed:\n","Hello world.\n","\n","Generated text:\n","Hello world. This is a simple transformer demormer d. This is a simple transformer demor del\n"]}],"source":["model.eval()\n","\n","start_text = \"Hello world.\"\n","start_idx = torch.tensor([[char_to_idx[ch] for ch in start_text]], device=device)\n","\n","generated_idx = model.generate(start_idx, max_new_tokens=80)[0].cpu().tolist()\n","generated_text = ''.join(idx_to_char[i] for i in generated_idx)\n","\n","print('Seed:')\n","print(start_text)\n","print('\\nGenerated text:')\n","print(generated_text)"],"id":"8yi5mtdVf19F"},{"cell_type":"markdown","metadata":{"id":"Di1ThPRSf19G"},"source":["## 8. Inspect token embeddings (\"character embeddings\")\n","\n","Like in real LLMs, each token (here: character) gets mapped to a dense vector. Let's inspect a few of them."],"id":"Di1ThPRSf19G"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIrDVVnjf19G","executionInfo":{"status":"ok","timestamp":1763394524410,"user_tz":420,"elapsed":45,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"37e77e6f-10f3-4fec-f7da-f29ab9512d98"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding matrix shape: torch.Size([19, 64])\n","\n","Character 'H' (index 2) embedding vector (first 10 dims):\n","tensor([-0.0954,  1.3438,  1.4395,  0.9203,  0.8983,  1.2144, -0.3915,  0.1463,\n","        -0.7597,  1.8002], requires_grad=True)\n","\n","Character 'e' (index 6) embedding vector (first 10 dims):\n","tensor([ 0.9669,  0.4964, -1.8346, -0.2491, -0.8514, -1.5402, -0.8203,  0.4805,\n","         0.2247, -2.4325], requires_grad=True)\n","\n","Character 'o' (index 13) embedding vector (first 10 dims):\n","tensor([ 0.6131,  2.3747, -0.3250, -0.6984,  0.1271,  1.7151,  0.3394,  0.6249,\n","         0.6121, -2.5168], requires_grad=True)\n","\n","Character ' ' (index 0) embedding vector (first 10 dims):\n","tensor([-0.1127, -0.1695, -1.0145,  0.0959, -0.6880,  0.1390,  1.1265,  0.3472,\n","        -0.5273, -0.3755], requires_grad=True)\n"]}],"source":["with torch.no_grad():\n","    emb_weights = model.embed.token_emb.weight  # [vocab_size, d_model]\n","    print('Embedding matrix shape:', emb_weights.shape)\n","\n","    for ch in ['H', 'e', 'o', ' ']:\n","        if ch in char_to_idx:\n","            idx = char_to_idx[ch]\n","            vec = emb_weights[idx]\n","            print(f\"\\nCharacter '{ch}' (index {idx}) embedding vector (first 10 dims):\")\n","            print(vec[:10])"],"id":"EIrDVVnjf19G"}]}